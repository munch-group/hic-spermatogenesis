---
title: "Chromatin Compartments and Selection on X"
subtitle: "How Edges of Active Chromatin Align with Selection Regions in Primates"
authors:
  - name: "Søren Jørgensen"
    affiliation: |
      Bioinformatics Research Center (BiRC), \\
      Dept. of Molecular Biology and Genetics, \\
      Aarhus University, Denmark
    roles: writing
    corresponding: true
    email: 201906763@post.au.dk
#  - name: "Kasper Munch"
#    affiliation: |
#      Ass. Prof. Bioinformatics \\
#      Bioinformatics Research Center (BiRC), \\
#      Dept. of Molecular Biology and Genetics, \\
#      Aarhus University, Denmark
#    roles: supervision
degree: MSc. Bioinformatics
keywords:
  - Hi-C
  - Chromatin Compartments
  - Selection on X
  - Selfish gene drive
abstract: |
  The hemizygosity of male mammals makes the X chromosome uniquely exposed to selective pressures, as the lack of a second copy provides no buffer against deleterious mutations. This, combined with a high density of essential genes related to reproduction and brain function, suggests the existence of biological mechanisms that protect the integrity of the X chromosome. One such mechanism is meiotic sex chromosome inactivation (MSCI) in males, while another could involve chromatin architecture. 
  
  In this study, the 3D chromatin architecture of the X chromosome in rhesus macaque (*Macaca mulata*) is investigated in the context of evolutionary pressures and genetic drivers. We redo Hi-C analyses from a 2019 paper on the latest reference genome (*rheMac10* or *Mmul_10*). We compare two Hi-C analysis frameworks, *HiCExplorer* and *cooler/cooltools* (Open2C), on a subset, finding Open2C to be most flexible and intuitive. The ICE method (Iterative Correction and Eigendecomposition) was used to infer conventional and refined A/B compartments for fibroblast and four stages of spermatogenesis. 
  
  We find that 200 kbp transition-zones between A/B-compartments in both fibroblasts and round spermatids align well with strong selective sweeps in humans (ECH-regions), and with strong negative selection in baboons (*Papio* spp.). We discuss the biological meaning of these findings, where conserved chromatin features may help to retain non-advantageous alleles, hinting to the role of selfish genetic elements in genome evolution.
plain-language-summary: |
  3D structure of chromatin brings another mystery onto the story of selfish genes, where the edges of A-compartments in macaque align well with regions of positive selection identified in human and moderately with negative selection in baboons.
key-points:
  - Compare two frameworks for Hi-C analysis, HiCExplorer and Open2C
  - Redo Hi-C analysis of rhesus macaque X chromosomes on the latest reference genome *rheMac10* 
  - Compartment edges align well with extended common haplotypes in human, suggesting a role of selfish genetic elements. 
date: last-modified
---

# Introduction

## Sexual reproduction and Sex Chromosomes

The production of gametes in a sexually reproducing organism is a highly complex process that involves numeruous elements. Spermatogenesis, the process of forming male gametes, involves four stages of differentiation from a germ cell through *spermatogonia*, *pachytene spermatocyte*, and *round spermatids* to *spermatozoa*, or *sperm* [@wang2019reprogrammingmeioticchromatin], and it is the very basis of male reproduction. The specialized cell division of meiosis neatly handles the pairing, recombination, and segregation of homologous chromosomes, thereby ensuring proper genetic distribution. Deeply understanding the steps of molecular steps of reproduction and how our genetic material is inherited is essential in biology, bringing insight to areas such as speciation, population diversity, and (male) infertility.

Sex chromosomes behave differently than autosomes for a long list of reasons. One reason is hemizygosity, where there is only one copy of a chromosome. The Y chromosome exist only in males, and (generally) never more than one copy is in the same individual. The copmlexity increases when we tend to the X chromosome, which exist both alone (in males) and as two copies (in females). This skewed ratio means that X chromosomes exist $2/3$ of the time in females and only 1/3 of the time in males, and it is more exposed in males as there is no other copy to take over loss of function. It is also the underlying reasoning for Haldane's rule \[ref\], postulating that the heterogametic sex (XY, ZW) is the first to dissapear or have severely lowered fitness (e.g. sterility) when crossing different species. Even today, a hundred years later, there is only hypotheses as to why this is observed, including *Y-incompatibility* (Y has to be compatible to X or autosomes), *dosage compensation* (hybridisation deregulates crucial dosage compensation in heterogametes), *dominance* (recessive genes causes sterility), *faster-male* (male reproductive genes diverge faster than female), *faster-X* (X-linked loci diverge faster than autosomal ones), and *meiotic drive* (discrepancy between drivers and surpressors on sex chromosomes leads to sterility) \[ref\]. While is appears to be the same phenomenon across multiple *taxa* (even *kingdoms*), it has received different explanations, and consensus for different species are not the same. Often times even, several of the listed explanations are described to be acting in collaboration \[ref\].

-   [ ] include something about conserved synteny on chrX here

The complexity of selection on the X chromosome therefore still an area of active investigation, and several studies infers selective strong selection on the X chromosomes across primates \[ref\]

## Known selection on X

-   [ ] Selective sweeps (or is it?) in humans (ECH90; @skov2023extraordinaryselectionhuman)
    -   [ ] Explain why it makes sense to compare these data sets
-   [ ] Negative selection in baboons w.r.t. minor parent ancestry
    -   [ ] Explain why it makes sense to compare these datasets
    -   [ ] The low-diveristy in *p.hamadryas* are very wide - maybe chromatin

## Gene drivers

-   [ ] Talk about what makes a genetic driver, what criteria

Gene drive occur when a particular collection of genes is propagated through a population by increasing the probability of transmitting the genes to the offspring from random (Mendelian) inheritance, resulting in a biased gene transmission against its alternative. Two categories of gene drivers exist [@bravonunez2018geneticvillainskiller]; *class one drivers* affect chromosome segregation in meiosis, and *killer meiotic drivers* will sabotage meiotic product that have not inherited the driving allele \[ref\]. This happens regardless of the fitness effects of the developed organism, and is there often a factor offsetting classical selection. Even though the implications of such systems are potentially detrimental, they are notoriously difficult to detect. A circumstance contributing to the difficulty is that most genetic experiments are done in homozygotes. @bravonunez2018geneticvillainskiller states that the general choice of experimental system may have biased our understanding of sexual reproduction. A key point is; meiotic drivers can only be observed in heterozygotes, where a genetic driver has a competitor.

Interestingly, gene drive is much more well-documented on sex chromosomes than for autosomes. Possibly because the sex chromosome meiotic drive inherently causes a skewed sex ratio, more notably raising a flag for further analysis. Another point to consider is, in the case of sex chromosome drive, a fully driving gene will lead to the extinction of the species \[ref\], as only one (fertile) offspring will be produced.

## Selfish genes (and randomness)

**The conventional story of meiosis in gametogenesis is one of random segregation of the sex chromosomes. They split into haploid gametes, where each chromosome has an equal chance of being passed on to a gamete. That seems like a fair game, but what if some genes are cheating the system by making others less viable. A meiotic driver is a selfish gene element that modulates meiosis and preferentially transmits its own allele through meiosis, regardless of the downstream fitness effects it may have (good or bad) on the organism it is part of. This phenomenon challenges the traditional understanding of selection, extending its scope beyond the fitness effects on an organism to include selective pressures at the molecular level. For example, if some genes on the X chromosome create a disadvantage for gametes that *do not* contain those genes, making sure the Y chromosome is not as viable as the X, resulting in a sex imbalance and possibly numerous other downstream effects. That is exactly what is coined *sex chromosome meiotic drive*[@jaenike2001sexchromosomemeiotic], a result of selfish genetic elements. Motivated by previous results in the Munch Research group [@munch2024munchgroup] on hybrid incompatibility and extended common haplotypes [@skov2023extraordinaryselectionhuman; @sorensen2023genomewidecoancestryreveals] that could be explained by meiotic drive, we wanted to investigate how these patterns correlate with chromatin compartments.**

## Chromatin Architecture

-   [ ] Chromatin organization is highly conserved between species
-   [ ] Explain why take offset in @wang2019reprogrammingmeioticchromatin
-   [ ] Exlpain the rationale of reproducing results with the latest reference
-   \[ \]

## 3C: Chromatin Conformation Capture

-   [ ] This might not be needed or will be merged with Hi-C section

## Hi-C: High-Throughput 3C

Our DNA can be divided into different orders of structure. *3C* focus on identifying the highest orders of organization inside the nucleus, that is, when the 30 nm thick coil of chromatin fibers folds into loops, Topologically Associating Domains (TADs), and chromatin compartments. Here, we narrow our focus on the largest of the structures, *compartments*, that is known to determine availability to transcription factors, thus making an *A* compartment *active*---and the *B* compartment *inactive*. The introduction of the Hi-C (high-throughput 3C) method [@lieberman-aiden2009comprehensivemappinglongrange] opened new possibilities for exploring the three-dimensional organization of the genome.

### Hi-C Library preparation

A specialized protocol for preparing the DNA library is necessary [@lieberman-aiden2009comprehensivemappinglongrange, Fig. 1a]. Briefly, formaldehyde is used to crosslink spatially adjecent chromatin. Restriction enzyme *HindIII* is used to digest the crosslinked chromatin, leaving sticky ends, `5-AGCT-3`, that are filled and biotinylated with a polymerase (using either biotinylated A, G, C, or T). The strands are ligated in highly dilute conditions, which is favoring the ligation of the two crosslinked strands, forming chimeric, biotinylated strands. Upon ligation, the restriction site is lost as a biotinylated `5-CTAG-3` site (also referred to as the *ligation junction*) is formed. Lastly, the ligation junctions are isolated with streptavidin beads and sequenced as a paired-end library.

To be able to create stage-resolved Hi-C library of spermatogenetis, several steps have to be performed on the samples before crosslinking. First, the samples have to be treated immediately after harvesting to ensure viable cells. Secondly, the samples have to be purified to accurately represent each stage og spermatogenesis. Specifically, the data for this project [@wang2019reprogrammingmeioticchromatin, acc. GSE109344] preluded the library preparation protocol by sedimentation-based cell sorting to separate live spermatogenic cells into different stages of differentiation, namely spermatogonia, pachytene spermatocyte, round spermatid, and spermatozoa. Then, the cells were fixed in their respective state before crosslinking. They use their own derived method for library preparation, termed small-scale *in-situ* Hi-C, allegedly producing a high-quality Hi-C libary from as little as 500 cells (capturing the variance of millions of cells).

Initially, Hi-C library preparation was designed to generate molecules with only a single ligation site in each, but with advancements in sequencing technology ('short-reads' can now span several hundreds of base pairs) and the shift to more frequently cutting restriction enzymes for higher resolution results in multiple ligation events per sequenced molecule [@open2c2024pairtoolssequencingdata], which is adressed in the section below. 

### Hi-C Data Analysis

The analysis of the read-pairs of a Hi-C library is divided into several smaller tasks, see \[ref fig-hic-analysis-flow\].

::: {#fig-hic-analysis-flow}
![](illustrations/fig-hic-data-analysis.png)

A simplified pipeline for Hi-C data analysis: from raw reads to a Hi-C interaction matrix. See @tbl-ligation-events for details on ligation events.
:::

We must align the reads to the reference in such a way that the *intentional* chimeric read-pairs (as per above-mentioned protocol) are rescued, and *unintentional* read-pairs are discarded. That is, we must make sure they represent ligation junction of adjecent chromatin segments, and not technical artefacts or unintentional (random) fusions of unrelated DNA.

#### Aligning the Hi-C reads

The main difference between Hi-C libraries and standard paired-end libraries is the high fraction of chimeric reads in Hi-C. As a contact pair is crosslinked and ligated before sequencing, chimeric reads occur as a feature, and standard mapping techniques seeks to filter out this type of reads \[ref\]. Thus, we need specialized tools for rescuing chimeric reads. That said, we have to be cautious distinguishing the intended chimerism for Hi-C and that of technical artefacts. Any software for local alignment can be used for aligning reads from a Hi-C library. However, one should make sure to disable paired-end rescue mode if possible, otherwise each read in a pair (each mate) should be aligned separately [@lajoie2015hitchhikersguidehic]. This removes the assumption that the distance between mates fits a known distribution because the genomic sequences originate from a continuous DNA-fragment. For example, the *bwa-mem* \[ref\] implementation of this (the `-P` option) activates the Smith-Waterman algorithm to rescue missing hits, but disables the search of hits that fit a 'proper' pair. After alignment, each read is typically assigned to the nearest restriction fragment to enable categorization of pairs into different categories.

Interestingly, this last step is not included by default in *pairtools*, as \[ref\] observe very similar statistical properties on pairs that are either close or distant from the nearest restriction site. Thus, restriction fragment filters are not needed, and in stead, a simple filter is applied against short-distance pairs that is automatically calibrated.

#### Identifying and Storing Valid Hi-C Pairs

One should be cautious when filtering invalid from valid pairs, as they are not easily distinguished. A ligation event will be categorized into one of five categories (see @tbl-ligation-events): *dangling-end*, *self-circle*, *weird*, *intrachromosomal* (cis), and *interchromosomal* (trans) [@bicciato2022hicdataanalysis, Ch. 1]. Either *dangling-end* or *self-circle* events are reported if a read-pair maps to the same restriction fragment depending on the orientation, and deemed uninformative [@lajoie2015hitchhikersguidehic]. Usually, *weird* events are demeed uninformative as well, as it is challenging to distinguish a sequencing error from the result of a diploid fragment. PCR duplicates should be discarded as well, having either identical genomic sequence, or sharing exact 5' alignment positions of the pair [@lajoie2015hitchhikersguidehic; @bicciato2022hicdataanalysis, Ch. 1]. The probability that such pairs are valid (i.e. there are multiple of the same pairs) is very low. We also have to distinguish between molecules with only a single ligation event (one-way contact) or multiple ligation events (multi-way contacts). For that, a descision should be made on whether to 1) discard molecules with multiple ligations, 2) report one of the ligations (e.g. the 5'-most in both directions), or 3) report all events on a molecule. 

\small

::: {#tbl-ligation-events .striped}
| Event name | Explanation |
|:--------------|:--------------------------------------------------------|
| Dangling-end | Non-digested collinear fragments. Fraction can be high. |
| Self-circle | Collinear fragment(s) have circularized. Very low fraction could indicate unsuccesful ligation. |
| Weird | Mates have the same orientation on the reference. Is not possible with single copy fragment. Either sequencing errors or diploid fragments[^1]. |
| Cis | Pairs from the same chromosome (intrachromosomal) |
| Trans | Pairs from distinct chromosomes (interchromosomal) |

Five categories of ligation events and a short explanation. *Hi-C Data Analysis: Methods and Protocols Ch. 1*.
:::

[^1]: @bicciato2022hicdataanalysis mentions that this type of ligations had been used to model interaction between sister-chromatids post-replication in *Drosophila*.

\normalsize

#### Quality Control and Interaction Matrices {#sec-matrix-qc}

To determine the quality of the Hi-C library, most tools generate quality control log files at some point during the filtering steps, which can then be aggregated and analyzed (with e.g. MultiQC \[ref\]). The ratios between the different ligation events can be informative about the quality of the Hi-C library. Here, both the distribution of discarded reads across categories, as well as the ratios between *cis*/*trans* interactions for a certain organism provide information about the library. For example, the biases of different aligners might be captured by comparing the reason why reads are discarded between two different aligners, as well as if there is a preference of *cis* or *trans* in an aligner itself. This allows for evaluating the mapping parameters as well as the filters applied downstream. Additionally, $P(s)$, the contact probability as a function of genomic separation can be inspected as it should decay with increasing distance. The *trans*/*cis*-ratio can sometimes be a good indicator of the noise level in the library, and additionally, the level of random ligation events can be quantified by counting the number of *trans* events occurring to mitochondrial genome. They should not occur naturally, as the mitochondrial genome is separated from the DNA in the nucleus. This method has some pitfalls that should be controlled for; some parts of the mitochondrial genome can be integrated into the host genome, and mitochondrial count may differ between cell-stages.

Typically, a filter against low mapping quality is applied on the data before constructing the interaction matrix (Hi-C matrix), and a conventional threshold is $mapq < 30$ [@bicciato2022hicdataanalysis]. However, a considerable amount of reads do not pass that threshold, and thus we risk discarding potential valid information and should make sure to have enough data. Consequently, *HicExplorer* defaults a lower threshold ($mapq < 15$), and *pairtools* enforces no filter by default, but recommends setting this manually (starting at $mapq < 30$).

A Hi-C interaction matrix simply maps the frequency of interactions between genomic positions in a sample. The maximum resolution of a Hi-C matrix is defined by the restriction enzyme, where the size of the restriction site (probabilistically) determines average space between each cut. With a 4 bp restriction site, the fragments will average $4^4 = 256 bp$ and similarly $4^6 = 4096 bp$ for a 6 bp restriction site. This leads to \~12,000,000 and \~800,000 fragments, respectively. Very deep sequencing is required to achieve enough coverage to analyze the interaction matrix at the restriction fragment resolution, but, usually, such high resolution is not required. Therefore, it is practice to bin the genome into fixed bin sizes, which also enables a more efficient handling of the data if the full resolution is not needed (e.g. when plotting large regions such as a whole chromosome). The conventional format to store a Hi-C matrix, consisting of large multidimensional arrays, is HDF5. Each HDF5 file can store all resolutions and metadata about the sample, resolutions typically ranging from 10kb to 1Mb. Typically, the stored resolutions should be multiples of the chosen base-resolution, as the lower resolutions are constructed by recursive binning of the base resolution. *cooler* \[ref\] neatly offers efficient storage with sparse, upper-triangle symmetric matrices and naming-conventions of the groups in their *.h5*-based file format, *.cool*, and they provide a Python class `Cooler` as well for efficiently fetching and manipulating the matrices in Python.

#### Inferring from the matrix (Calling Compartments)

The raw frequency matrices are generally not very informative, as the contact frequencies vary greatly between bins and contain biases in addition to the $P(s)$ decay, which results in a diagonal-heavy matrix with high amount of noise the further we travel from the diagonal. Therefore, to analyze the three-dimensional structure of the chromatin, a method for correcting (or balancing) the raw Hi-C matrix has to be applied. It is unadvisable to correct low-count bins as it will greatly increase the noise, or to correct very noisy bins, or very high-count bins. Therefore, some bin-level filters are applied before balancing [@lajoie2015hitchhikersguidehic];

-   Low-count bins are detected by comparing bin sums to the distribution of bin sums with a percentile cutoff,
-   Noisy bins are detected by comparing bin variance to the variance distribution of all bins (and percentile cutoff), and
-   Outlier point-interactions are removed (a top-percentile of bin-bin interactions)

A widely used balancing method is Iterative Correction and Eigendecomposition (ICE) [@imakaev2012iterativecorrectionhic], which utilizes a data-driven approach for correcting multiplicative biases. Briefly, is based on an assumption of equal visibility of all loci, and uses the pairwise and genome-wide structure to generate a set of biases along with a map of relative interaction frequencies by iteratively dividing each row, then each column, by its mean until convergence. This results in a uniform coverage profile (corrected coverage), yielding a smoother interaction matrix with slower transitions, thus greatly reduces visibility-induced biases. It does not distinguish between the sources of biases, and thus calculates a collective bias for each position. @imakaev2012iterativecorrectionhic show that *known* biases are factorizable by comparing their results to predictions of restriction fragment biases, GC content, and mappability from a computationally intensive probabilistic approach. By showing that the product of those known biases explain $>99.99%$ of the variability in their bias estimation, they argue both known and unknown biases will be captured with their iterative correction method (also denoted *matrix balancing*).

Even with a binned, filtered, and balanced matrix, we are still left with the challenge of translating the matrix into biologically relevant inferations. Importantly, we have to remember that the matrix arise from a collection of cells and that the interaction frequency cannot be translated to a fraction of cells. Additionally, the effect from averaging interaction patterns can cause both individual patterns to be burried and the average pattern to show a pattern that does not exist in any of the single cells. Therefore, when pooling matrices one must make sure that the samples are as similar as possible (e.g. the same differentiation stage and so on). We can also not distinguish interactions that either co-occur in the same cell or ones that are mutually exclusive. Lastly, the way interaction patterns are defined poses a challenge; we define the chromatin compartments to be the output of a method, the 'E' in 'ICE', eigendecomposition, not as a specific pattern that we can explicitly search for. Although experimentally verified to tightly correlate with chromatin states, the inferred compartments vary with different methods of calculating the eigenvector, as dicussed in @sec-methods-eigendecomposition and @sec-results-eigenvectors. To further complicate the challenge, interaction patterns on different scales co-exist and are difficult to disentangle without simplifying assumptions such as small-scale interactions are not visible (or they are negligible) at a certain resolution, or restricting the viewframe to eliminate large-scale variance between chromosome arms. It is by definition a speculative exercise to interpret the biological relevance of an observed pattern, but the consensus is to call compartments on interacting regions that arise from the eigendecomposition of a Hi-C matrix without further modifications [@lajoie2015hitchhikersguidehic]. As the eigenvector is only unique up to a sign, a phasing track is used to orient the eigenvector, aiming for a positive correlation with GC content (in mammals), making A-compartments represent the active euchromatin, and B-compartments the closed heterochromatin.

#### Compartment Edges and Genomic Intervals

As arbitrary as a compartment may be defined, we chose to define another genomic interval for analysis. It is well [@bicciato2022hicdataanalysis, Ch. 3] known that CTCF and other structural proteins preferentially binds to Topologically Associating Domains (TADs; they were initially defined as sub-Mb chromatin structures [@lajoie2015hitchhikersguidehic], but now the definition seems to vary based on the method of extraction \[ref cooltools\]). Derived from this, we define a transition zone between A/B compartments to look for enrichment of specific regions of interest.

We can test if two sets of genomic intervals correlate (say, compartment edges and ECH regions) by either proximity of the non-intersecting parts of the sets, or by intersection over union (Jaccard index). When the underlying distribution of a statistic (or index) is unknown, a widespread method in bioinformatics for estimating a p-value is by bootstrapping. Here, one of the sets are bootstrapped (the intervals are placed at random positions) a number of times, $b$, and the fraction of statistics more extreme than the one we observe is reported as the p-value.

## Reproducibility Infrastructure

**First, describe the importance of reproducibility and the scientific method. Include a bit of scientific skeptiscism of there is time.**

Thus, apart from the biological questions we seek to investigate and answer in this thesis, a major goal of the thesis is to create fully (and easily) reproducible results through a self-contained and version-controlled pipeline using git \[ref\], GitHub \[ref\], quarto \[ref\], Conda \[ref\], gwf \[ref\], and Jupyter \[ref\]. See @tbl-reproducibility for a brief introduction.

\small

::: {#tbl-reproducibility .striped}
| Tool | Description |
|:-------------|:---------------------------------------------------------|
| Jupyter | Interactive coding environment for analysis and development (notebooks are natively rendered with Quarto) |
| Quarto | A Quarto Manuscript project nested inside a Quarto Book for rendering html (website) and PDF (manuscript) from Markdown via Pandoc. Supports direct embedding of output from Jupyter Notebook cells (plots, tables). |
| Conda | For managing software requirements and dependency versions reproducibly. |
| git | Version control and `gh-pages` branch for automated render of Quarto project |
| GitHub | Action was triggered `on push` to render the project and host on [munch-group.org](https://munch-group.org/hic-spermatogenesis/) |
| *gwf* | Workflow manager to automate the analysis on a HPC cluster, wrapped in Python code. `workflow.py` currently does everything from `.fastq` to `.cool`, but notebooks can be set to run sequentially as part of the workflow as well. |

Overview of the tools used for reproducibility of this thesis.
:::

\normalsize

### GWF: workflow management for High-Performance Computing (HPC)

To enable consistently reproducing the analyses, a workflow manager is used. Several exist, but the most well-known must be *snakemake* \[ref\]. However, we use the pragmatic (their own words), lightweight workflow manager *gwf*, which is optimized for the GenomeDK insfrastructure, and has the benefit of in-house support.

Briefly, *gwf* works on a python script, conventionally `workflow.py`, that wraps all the jobs (*targets* in gwf lingo) you will submit to the HPC cluster. Each target is submitted from a template, written as a Python function, which includes `inputs` and `outputs` that *gwf* should look for when building the depency graph, `options` list of resources that is forwarded to the queueing system (*Slurm* in our case), and `specs`, specifying the submission code in Bash as a formatted Python string (meaning we can pass Python variables to the submission code), providing an extremely flexible framework for running large and intensive analyses in a high-performance computing environment.

### Project Initialization

#### *gwf*

The initialization of the project directory is the basis of reproducibility and transparency, together with `workflow.py` inhabiting the main directory. Specifically, it includes a subdirectory for (intermediate) files that are produced by the pipeline, `steps/`. Everything in this directory is reproducible simply by re-running the *gwf*-workflow. It is thus not tracked by `git`, as the large files (raw reads, aligned read-pairs, etc.) it contains are already indirectly tracked (`workflow.py` is tracked). It can be safely deleted if your system administrator tells you to free up disk space, although you would have to run the workflow again to continue the analysis. Several directories are created for files that are not produced by the pipeline, that is, files that the workflow uses, configuration files, figures edited by hand, etc. Ideally, as few files as possible should be outside of `steps/`, to be as close as possible to an automated analysis.

#### Jupyter Notebooks

A `notebooks/` subdirectory contains Jupyter notebooks that are named chronologically, meaning they operate on data located in either `steps/` or generated from a previous notebook. This way, the workflow can also be set up to run the notebooks (in order) to produce the figures, tables, and their captions used in this manuscript.

#### Quarto

Quarto is an open-source scientific and technical publishing system that uses (pandoc) markdown to create and share production quality output, integrating Jupyter Notebooks with Markdown and LaTeX and enabling embedding content across *.ipynb* and *.qmd*. In *.qmd*, code chunks in several programming languages can be executed and rendered, including Python, R, mermaid (JavaScript-based diagramming). A Quarto project is configured with a YAML configuration file (`_quarto.yml`) that defines how output is rendered. In this project, we use a nested structure, nesting a *Slides* project and a *Manuscript* project inside a *Book* project. To manage the directory as a Quarto Book project, a quarto configuration file was placed at the base, defining how the Book should be rendered. Additionally, configuration files were placed in `slides/` and `thesis/`, to render them as Quarto Slides and Quarto Manuscript, respectively. This nested structure lets us render different subprojects with different configurations than the main project, for example to generate the manuscript, a single Quarto Markdown file, in both *.html* and *.pdf*, and only including embedded outputs from specified cells from notebooks in the parent directory. Although the Quarto framework is extensive, it is still under development and has several drawbacks worth mentioning. First, one can only embed the output of code cells from notebooks, meaning the only way to embed text with a python variable (e.g. you want the manuscript to reflect the actual value of a variable, sample sizes `n = [1000, 10000, 100000]`, and their respective outputs) is by converting a formatted python string into Markdown and send it to the output. Second, embedded figures will be copied as-is in the notebook, and thus cannot be post-processed with size or layout. This makes it impractical to e.g. use the same figures in slides and in the manuscript. Third, when rendering large projects that is tracked by git, some output files (that have to be tracked to publish the website) can exeed GitHub size limits. Especially if rendering in the *jats* format, producing a MECA Bundle that should be the most flexible way to exchange manuscripts \[ref\]. However, as not applicable to this thesis, the option was simply disabled. Fourth, some functionality relies on external dependencies that cannot be installed on a (linux) remote host (GenomeDK), such as relying on a browser for converting `mermaid` diagrams into png for the pdf-manuscript.

### git and GitHub

To track the project with git and GitHub, the abovementioned structure was initialized as a GitHub repository, including a workflow for GitHub Actions to publish and deploy the website on the `gh-pages` branch when pushing commits to `main`. Briefly, it sets up a virtual machine with Quarto and its dependencies, renders the project as specified in the `_quarto.yml` configuration file(s), and publishes the project on the group website [munch-group.org](https://munch-group.org/hic-spermatogenesis).

## Our research question

In this project, we formulate two main objectives:

#### A

Redo the Hi-C analyses from [@wang2019reprogrammingmeioticchromatin] using the latest macaque reference genome, *rheMac10*, with some modifications. We decided to use *HiCExplorer*, a Python-based software for command line use, and supplement the analyses with the *Open2C Ecosystem* [@openchromosomecollective] that have a Pyton API as well as command-line functions, which can be paired very well with Jupyter Notebooks. The majority of the data analysis was run with a *gwf* workflow, and the commands that were visually inspected were run in Jupyter Notebooks.

#### B

Compare with regions of extended common haplotypes (strong selective sweeps) that are found in *human*, and with regions of negative selection of minor parent ancestry in baboons. Investigate the biological meaning of the results. We use in-house software to compare genomic intervals.

# Methods

All computations were performed on GenomeDK (GDK) \[ref\], an HPC cluster located on Aarhus Uninversity, and most of the processing of the data was made into a custom *gwf* workflow \[ref\], a workflow manager developed at GDK. I would like to thank GDK and Aarhus University for providing computational resources and support that contributed to these research results.

With the analysis tools determined in the above section, we decided it was not feasible to follow the exact approach as @wang2019reprogrammingmeioticchromatin with any of *HiCExplorer* and *Open2C*, as they use a third software, *HiC-Pro*. For mapping the raw reads, Hic-Pro internally uses bowtie2 in end-to-end mode, followed by trimming the 3'-end of the unmapped reads, then remapping the 5'-ends to rescue chimeric fragments. I mapped the reads using `bowtie2 --end-to-end` without the rescue-remapping, and it returned a very high fraction of discarded reads, and it would be silly to spend time on implementing the remapping approach manually. When redoing analyses, it is not sensical to use methods that are not state-of-the-art, and judged by the time since last release (HiC-Pro v3.1.0 in 2021), both HiCExplorer and Open2C are more recent. Additionally, the HiC-Pro pipeline stops at a normalized contact map, and is thus not sufficient for downstream analysis. In hindsight, it would have been more sensical to use *HiC-Pro* to get normalized contact maps, then continue analyzing with *cooler*/*cooltools*, then compare the results evenly with the results achieved from using Open2C from start to finish. @fig-hic-tools-comparison gives an overview of the 3 pipelines metioned in this report.

::: {#fig-hic-tools-comparison}
![](illustrations/placeholder2000x360.png){width="50%" height="300%"}

A 3-column flow of HiC-Pro, HiCExplorer, and Open2C
:::

## Fetching raw data

To reproduce the results from [@wang2019reprogrammingmeioticchromatin], I chose to use their raw data directly from the SRA portal [ref]. I filtered the data to contain all their paired-end Hi-C reads, and included only macaque samples. The data set also contains RNAseq data, and the same tissues for both macaque and mouse. The meta data for the data set was extracted into a runtable `SRA-runtable.tsv`. To get an overview of the data accessions used in this analysis, we will first summarize the runtable that contains the accession numbers and some metadata for each sample (@tbl-runtable-summary). It adds up to \~1Tb of compressed `fastq` files, holding \~9.5 billion reads, roughly evenly spread on the 5 tissue types.

\small

{{< embed ../notebooks/03_compartments.ipynb#tbl-runtable-summary >}}

\normalsize

### Fetching and indexing the reference

@wang2019reprogrammingmeioticchromatin use the 2006-version of the macaque reference, *rheMac2*. Supporting my previous sentiment about not using outdated resources I find it reasonable to use the latest reference, *rheMac10*. @warren2020sequencediversityanalyses have improved contiguity from *rhemac8* by 120 fold, going from N50 contig size of 107 Kbp to 46 Mbp. Part of the reasoning for reproducing their results was doing so on the latest assembly of the *Macaca mulata* genome, which arguably will result in a more accurate mapping of the reads, and a better inference of the chromatin compartments as well.

Therefore, the latest reference genome for rhesus macaque/*Macaca mulata*, *rheMac10*/*Mmul_10* (UCSC or NCBI naming conventions, respectively) was downloaded to GDK from UCSC web servers with `wget`. To use `bwa` for mapping, rheMac10 needs to be indexed with both `bwa index` with the `--bwtsw` option and `samtools faidx`, which results in six indexing files for `bwa mem` to use.

Several mappers were used in different configurations (described in below), and `bowtie2` requires its own indexing of the reference, using `bowtie2-build --large-index`, which creates six index files for `bowtie2` to use. `--large-index` creates the special indexing format required for large genomes such as macaque.

## HiCExplorer trials

To get aligned reads in a format compatible with HiCExplorer, the read mates have to be mapped individually to the reference genome. This supports the old convention to avoid the common heuristics of local aligners used for regular paired-end sequencing libraries (@lajoie2015hitchhikersguidehic). HiCExplorer provide examples for both *bwa* and *bowtie2*, so I used both with recommended settings. *bowtie2* was more resource-intensive, and only succesfully aligned a small fraction of the reads \[ref sup-fig-bowtie2-stats\], but likely some parameters could be tuned for better alignment. In both cases, the aligner outputs a .bam-file for each mate (`sample_R1.bam` and `sample_R2.bam`), and HiCExplorer performs the parsing, deduplication, and filtering of the reads and builds the raw interaction matrix in a single command,

`hicBuildMatrix -s sample_R1.bam sample_R2.bam -o matrix.h5 [...]`,

For parsing, the command needs a `restrictionCutFile`, locating the restriction sites from the restriction enzyme used on the reference genome, which is generated with `hicFindRestSites` that operates on the reference genome and restriction sequence. The default filter, `--minMappingQuality 15`, was applied as described in @sec-matrix-qc. Notably, HiCExplorer has no options on handling multiple ligations, and thus the method is unknown. I assume that they have the intitial design of Hi-C libraries in mind. 

::: {#fig-hicexplorer-workflow}

![](illustrations/placeholder2000x360.png){width="70%" height="100%"}

Overview of the target templates used for hicexplorer. As most operations are handled by `hicBuildMatrix`, it is rather simple. 
:::

\small

{{< embed ../notebooks/01_hicexplorer.ipynb#tbl-hic-exploration >}}

\normalsize

For the initial exploration of methods with *HiCExplorer*, we chose five fibroblast samples (see @tbl-hic-exploration). The goal was to replicate some of the figures from @wang2019reprogrammingmeioticchromatin using *HiCExplorer*, especially to reconstruct interaction matrices and E1 graphs from macaque data. We constructed matrices with `hicBuildMatrix` as described from the separately mapped read-pairs. Along with the matrix *.h5* file, a *.log* file was created as well, documenting the quality control for the sample. Multiple logs were aggregated and visualized with `hicQC`.

Before correction (or balancing) of the interaction matrix, a pre-correction filter is applied, filtering out low-count bins and very high-count bins. A threshold for Mean Absolute Deviation (*MAD*) is estimated by `hicCorrect diagnostic_plot`, followed by iterative correction with `hicCorrect correct --correctionMethod ICE`. The PCA was performed with `hicPCA` on the corrected matrices, yielding the first 3 PCs. 

`hicPlotMatrix` plots matrices directly to .png (no display). When keeping the analysis in a Jupyter Notebook (using the builtin shell-escape commands to execute `bash` code), the plot files must be embedded back into the notebook. There is limited support for modifying the plot (from command-line options), such as to add spacing for a bigWig track with E1 values, add plot titles, and define the size and resolution of the plot. I briefly tried to implement a plotting function on the .h5 matrices and bigWig tracks, but it could not fetch regions from a matrix on the fly and had to load the full matrix into memory (that is, all full-length chromosomes).

## Open2C pipeline

::: {#fig-flowchart-handling-coolers}
![](illustrations/placeholder2000x360.png)

Showing the *gwf* target templates used with the *Open2C* pipeline. As it is highly modular, it is also a bit elaborate. 
:::

A *gwf* workflow was created to handle the first part of the data processing, and each accesion number (read pair, mate pair) from the Hi-C sequencing was processed in parallel, so their execution was independent from each other.

#### Downloading the reads

The reads were downloaded from NCBI SRA with SRA-toolkit [@devteam2024sratoolkit] directly to GDK using a docker image of `sra-downloader` [ref *wwydmanski/sra-downloader*] as gunzipped `.fastq` files. Although possible to provide a list of accessions to the toolkit, I submitted each accession as a separate target, as `SRA Toolkit` acts sequentially, and only starts the next download after all compression tasks were done. It was therefore a low-hanging fruit to parallelize the download for efficiency. 

#### Mapping Hi-C reads

Suspiciously, [@openchromosomecollective] never mentions any problems with aligning the Hi-C reads, they just provide an example using `bwa mem` in paired-end mode and with the `-P` option set, which activates the Smith-Waterman \[ref\] algorithm to rescue missing hits, by focusing on assigning only one of the mates to a good mapping and escape mate-rescue. The documentation of `bwa` [ref](https://bio-bwa.sourceforge.net) state that both bwa-mem and bwa-sw will rescue chimeric reads. Consequently, Open2C does not have a builtin way of pairing the reads after mapping, and I was left with two options: **1)** to re(-)pair the individually mapped read-mates (.bam) with `samtools-fixmate` into one of the specific input formats required for `cooler` to create an interaction matrix *cooler*, or **2)** re-map the reads using Open2C's recommendations and use their established pipeline for producing a cooler. I chose the latter, where I mapped the fastq files to *rheMac10* in paired end mode for a pair ($m1$, $m2$) with `bwa mem -SP rheMac10 m1 m2`.

#### Parse and sort the reads

We need to convert the alignments into ligation events, and distinguish between several types of ligation events. The simplest event is when each side only maps to one unique segment in the genome 'UU'. Other events, where one or both sides map to multiple segments or the reads are long enough (\>150bp) to contain two alignments (multiple ligations) have to be considered as well. Multiple ligations (reads that have multiple ligation sites, thus having several valid alignments to the reference) are called *walks* by Open2C, and are treated according to the `--walks-policy` when parsing the alignments into valid pairs (or valid Hi-C contacts). Here, `mask` is the most conservative and masks all complex walks, whereas `5unique` and `3unique` reports the 5'-most or 3'-most unique alignment on each side, respectively, and `all` reports all the alignments. The pairs are piped directly into `pairtools sort` after parsing, as the deduplication step requires a sorted set of pairs. The *.pairs*-format produced by `pairtools` is an extension the [4DN Consortium](https://data.4dnucleome.org/file-formats/pairs/)-specified format, storing Hi-C pairs as in @tbl-pairsformat.

\small

::: {#tbl-pairsformat .striped}
| Index | Name | Description |
|-----------:|:-----------|:------------------------------------------------|
| 1 | read_id | the ID of the read as defined in fastq files |
| 2 | chrom1 | the chromosome of the alignment on side 1 |
| 3 | pos1 | the 1-based genomic position of the outer-most (5’) mapped bp on side 1 |
| 4 | chrom2 | the chromosome of the alignment on side 2 |
| 5 | pos2 | the 1-based genomic position of the outer-most (5’) mapped bp on side 2 |
| 6 | strand1 | the strand of the alignment on side 1 |
| 7 | strand2 | the strand of the alignment on side 2 |
| 8 | pair_type | the type of a Hi-C pair |
| 9 | mapq1 | mapq of the first mate |
| 10 | mapq2 | mapq of the second mate |

Column specification of the .pairs format as extended by `pairtools` [ref].
:::

\normalsize

I initially used `--walks-policy mask` without fully understanding the implications, but knowing it was the most conservative of the options. Only later I realized the recommendations from *pairtools*, specifically informing that longer reads ($>150bp$) might have a significant proportion of reads that contain complex walks. With this in mind and as the average read-length of our data is 300 bp, I decided to re-parse the alignments into a new set of pairs, and equally apply the recommended filter (next section). As both results are saved, we can compare the two approaches.

#### Filter and deduplicate pairs

Pairtools comes with a de-duplication function, `dedup`, to detect PCR duplication artefacts. At this point we will remove all reads that are mapped to an unplaced scaffold. Even though the publication of *rhemac10* assembly states they have closed 99% of the gaps since *rhemac8* \[ref\], *rheMac10* still contain more than 2,500 unplaced scaffolds, which are all uninformative when calculating the chromatin compartments as is the goal of this analysis. Therefore, we simply only include the list of conventional chromosomes (1..22, X, Y) when doing the deduplication. Initially, the default values were used to remove duplicates, where pairs with both sides mapped within 3 base pairs from each other are considered duplicates. `cooler` recommend to store the most comprehensive and unfiltered list of pairs, and then applying a filter it on the fly by piping from `pairtools select`. Initially, I missed this step and I did not filter for mapping quality. After reparsing the alignments and applying the same analysis, we compare the two pipelines. A quality control report is generated by `pairtools dedup` as well, and the reports are merged and visualized with `MultiQC` \[ref\] for each cell type.

#### Create interaction matrices (coolers)

The final part of the *gwf* workflow takes `.pairs` as input and outputs a `.cool` file (*cooler*). Initially, we read directly from the newly generated deduplicated pairs without additional filtering, but the official recommendation is to filter out everything below $mapq = 30$ by piping the pairs through `pairtools select "(mapq1>=30) and (mapq2>=30)"` to `cooler cload pairs`. I then re-parsed the alignments and created new coolers, including only the Hi-C contacts where $mapq \leq 30$, following the current recommendations from *cooler*.

#### Pooling samples (Merging coolers)

The samples are grouped into *replicates* with a unique **BioSample** ID, but we chose to pool all the interaction matrices for each cell type. We reason that when @wang2019reprogrammingmeioticchromatin determine compartments to be highly reproducible between replicates, by merging the replicates we can get a more robust signal.

`cooler merge` was used to merge all samples in each cell-type directory to just one interaction matrix for each cell type. The function merges matrices of the same dimensions by simply adding the interaction frequencies of each genomic position together, resulting in less empty or low-count bins.

#### Create multi-resolution coolers (zoomify)

A feature of working inside the ecosystem of *Open2C* \[ref\] is that it natively provides support for storing sparse interaction matrices in multiple resolutions in the same file by adding HDF5-groups to the (multires-)cooler. We can then efficiently store resolutions (i.e., different bin sizes) that is multiples of the smallest bin size. We chose to use 10kb, 50kb, 100kb, and 500kb bins, and the resolutions are made by recursively binning the base resolution. They call this process zoomifying, and `cooler zoomify` does the job (it recursively calls `cooler coarsen` to merge bins).

#### Matrix balancing (Iterative correction)

Finally, we balance (or correct) the matrices using the cooler CLI. We use `cooler balance` with the default options which iteratively balances the matrix (Iterative Correction). 

We balance the matrices on each resolution, and thus it cannot be done prior to zoomifying. They (@abdennur2020coolerscalablestorage) state that the balancing weights are resolution-specific and will no longer retain its biological meaning when binned with other weights. Therefore, we apply `cooler balance` to each resolution separately. `cooler balance` will create a new column in the `bins` group of each cooler, `weight`, which can then be included or not in the downstream analysis. This means we will have access to both the balanced and the unbalanced matrix.

The default mode uses genome-wide data to calculate the weights for each bin. It would maybe be more suitable to calculate the weights for *cis* contacts only, and that is possible through the `--cis-only` flag, and that can be added to another column, so that we can compare the difference between the two methods easily. However, when adding the option, the process seemed to stall and had to be terminated manually, and it was not investigated further.

#### Eigendecomposition {#sec-methods-eigendecomposition}

The eigendecomposition of a Hi-C interaction matrix is performed in multiple steps. As value of the eigenvector is only *significant* up to a sign, it is convention \[ref\] to use GC content as a phasing track to orient the vector. E1 is arbitrarily defined to be positively correlated with GC content, meaning a positive E1 value signifies an active chromatin state, which we denote a A-type compartment (or simply A-compartment). We performed eigendecomposition of two resolutions, 100 Kbp and 500 Kbp. @wang2019reprogrammingmeioticchromatin briefly describes their method to calculate the eigenvectors as a sliding window approach on the observed/expected matrix in 100 kb resolution summing over 400 kb bins with 100 kb step size, a method I was not able to replicate in the *Open2C* ecosystem. I decided to mimic this by smoothing the 100 kb E1 values by summing to 500 kb bins in steps of 100 kb, yielding a comparable resolution which I denote '*pseudo*-500 kb' resolution (*ps500kb*).

First, we calculate the GC content of each bin of the reference genome, *rheMac10*, which is binned to the resolution of the Hi-C matrix we are handling. It is done with `bioframe.frac_gc` (*Open2C*). To calculate the E1 compartments, we use only within-chromosome contacts (*cis*), as we are not interested in the genome-wide contacts. `cooltools.eigs_cis` will decorrelate the contact-frequency by distance before performing the eigendecomposition. `eigs_cis` needs a *viewframe* (view) to calculate E1 values, the simplest view being the full chromosome. However, when there is more variance between chromosome arms than within arms, the sign of the first eigenvector will be determined largely by the chromosome arm it sits on, and not by the chromatin compartments. To mitigate this, we apply a chromosome-arm-partitioned view of the chromosome (as a bedlike format, described in `bioframe` docs \[ref\]).

Additionally, to mimic the *Local PCA* from [@wang2019reprogrammingmeioticchromatin], I also defined a view of 10 Mb bins. Thoughout the project, I will compare results from each of the three views and resolutions.

#### Plotting matrices

We use matplotlib and seaborn to plot in the *Open2C* framework. Utilizing the `cooler` class, we can fetch regions of the matrix without modifying the file. As my analysis is centered around the X chromosome, it is efficiently handled by simply fetching 'chrX' from the matrix with `cooler.Cooler.matrix().fetch('chrX')`. Many methods of the cooler class returns data selectors, which do not retrieve data before it is queried \[ref\]. This means we can create many selectors at once without overflowing memory, enabling us to plot multiple interaction matrices side-by-side, e.g. the corrected and un-corrected matrices. This is easily done with the `balance` parameter of the matrix selector (`.matrix()`), which determines if it should apply the balancing weights to the coordinates and defaults to `True`.

The matrix is retrieved an plotted with `matplotlib.pyplot.matshow`, which automatically produces a heatmap image of the matrix. Here, in stead of transforming the interaction matrix, the color scale is log-transformed with `matplotlib.colors.LogNorm`. Additionally, `cooltools` comes with more tools to aid visualization: *adative coarsegrain* and *interpolation*, which can be chained. `adaptive_coarsegrain` iteratively coarsens an array to the nearest power of two and refines it back to the original resolution, replacing low-count pixels with NaN-aware averages to ensure no zeros in the output, unless there are very large regions that exceed the `max_levels` threshold, such as the peri-centromeric region.

I implemented a plotting utility, `plot_for_quarto` in notebook `07_various_plotting.ipynb` that is compatible with the YAML cell-options read by Quarto's `embed` shortcode. It will take an arbitrary number of samples and plot a chromosome (or region) with or without its respective E1 value for either of the three viewframes that has been created. The input is a (subsetted) *pandas DataFrame*, defined from a file search matching a pattern specified to the `glob` Python module.

### Compartments and Their Edges (Transitional Regions)

From the eigenvectors, the A-compartments were extracted in bedgraph-format (`['chrom', 'start', 'end']`) and compared with ECH90 regions lifted to *rheMac10* from human \[ref what reference?\]. We perform visual inspection of the genomic intervals and test whether ECH90 regions are enriched near the edges of the compartments by defining a 200 kilobase transition-zone centered at each sign change of E1 (referred to as *compartment edge*). We compare genomic intervals (or sets) both visually by plotting the regions, and by a proximity test and bootstrapping the Jaccard index.

#### Proximity test

Determines whether the non-overlapping parts of the sets are more proximal than expected by chance. We define the *annotation* set and the *query* set, and the distance from each interval on the *query* to the most proximal interval on the *annotation* is used to generate an index of proximity by the mean distance to nearest interval in the *annotation*. Then, bootstrapping ($b = 100000$) is performed by randomly *re*placing the query intervals to generate the null distribution, and finally, the fraction of the *null* as or more extreme as our observed proximity is reported as the p-value.

#### Jaccard test

Measures the significance of the observed Jaccard index (intersection over union) between two sets. The index is a measure of *similarity* ($intersection/union$) between two sets (between 0 and 1), which is very sensitive to the size difference between the sets, as even when comparing a set of intervals to a small subset of itself will yield a very small Jaccard index. When we use bootstrapping to generate a null distribution (shuffling the intervals of the *query*), we find the probability that the two sets (with their respective number and size of intervals), are as similar or more than what we observe. The ratio is reported as the p-value. However, this approach is still sensitive to flipping of query/annotation (if the reginos are not the same size), as only the query is bootstrapped.

#### Multiple testing

Careful considerations were made to avoid multiple testing biases (p-hacking): Performing tests on all combinations of variables (cell type, resolution, viewframe, flip annot, query) will yield 180 p-values for each test, and we would have to adjust the significance threshold (with $\alpha = 0.05$, we expect 9 'significant' tests by chance alone). However, if we test only a few combinations we will greatly reduce that. 

# Results

## HicExplorer Trials

### Quality Control

The separately mapped read-mates were parsed into a *.h5* interaction matrix by `hicBuildMatrix`, which include a *.log* file documenting the builtin quality control (hereafter, *QC*). Log files from the 5 samples were merged with `hicQC` (@fig-explorer-qc). We observe showed equal fractions of the read-orientation of read-pairs (@fig-explorer-read-orientation), which is expected for a good Hi-C library. Additionally, it determines between 40% to 50% of the total reads to be valid Hi-C contacts (@fig-explorer-unique-pairs), which is usually only 25%-40% (as described in HiCExplorer docs). @fig-explorer-contact-distance shows, however, unusually high fractions of *inter*-chromosomal contacts (up to 30%) compared to *intra*-chromosomal contacts (also denoted *trans* and *cis* contacts, respectively). It is expected that *cis* contacts are orders of magnitude more frequent than *trans* contacts [@bicciato2022hicdataanalysis, p. 236; @lieberman-aiden2009comprehensivemappinglongrange], and HiCExplorer states it should be below 10% (docs). The high fraction may be mitigated by enforcing a stricter *mapq* threshold for a valid Hi-C pair, as we also observe higher-than expected valid contacts. However, we continue without the current matrices.

::: {#fig-explorer-qc layout-ncol="3"}
![Read orientation](../steps/bwa/QC_all_samples/read_orientation.png){#fig-explorer-read-orientation}

![Unique pairs](../steps/bwa/QC_all_samples/unmappable_and_non_unique.png){#fig-explorer-unique-pairs}

![Discarded pairs](../steps/bwa/QC_all_samples/pairs_discarded.png){#fig-explorer-discarded-pairs}

![Pairs sequenced](../steps/bwa/QC_all_samples/pairs_sequenced.png){#fig-explorer-pairs-sequenced}

![Contact distance](../steps/bwa/QC_all_samples/distance.png){#fig-explorer-contact-distance}

Quality control of the mapped Hi-C reads using *HiCExplorer* `hicQC`. The figures should be moved to Supplementary/Appendix because they are ugly and un-alignable. But that is the fault of HiCExplorer, not me.
:::

### Correction

The correction diagnostic tool yielded a similar *mad* threshold within the range $[-3,-2]$. Even so, I followed the *HicExplorer* recommendation to set the lower threshold to at least -2 and the upper threshold to 5 in the pre-normalization filter. I argue that with a high number of valid contacts, it is safer to err on the side of caution and maybe filter out bad data.

::: {#fig-explorer-pre-correction layout="[[-5,25,25,25,-5], [-17.5,25,25,-17.5]]"}
![SRR6502335](../figures/bwa/SRR6502335_diag_plot.png){#fig-explorer-pre-correction-SRR6502335}

![SRR6502336](../figures/bwa/SRR6502336_diag_plot.png){#fig-explorer-pre-correction-SRR6502336}

![SRR6502337](../figures/bwa/SRR6502337_diag_plot.png){#fig-explorer-pre-correction-SRR6502337}

![SRR6502338](../figures/bwa/SRR6502338_diag_plot.png){#fig-explorer-pre-correction-SRR6502338}

![SRR6502339](../figures/bwa/SRR6502339_diag_plot.png){#fig-explorer-pre-correction-SRR6502339}

Histograms of the number of counts per bin (bottom x-axis) and the modified z-score (top x-axis) from which the *mad* threshold is defined.
:::

To compare these mappings with others, the QC results is an easy way. Therefore, the reads were mapped with *bowtie2* in both end-to-end- and local-mode followed by `hiCBuildMatrix`, and the QC from each method was plotted next to each other (@fig-explorer-all-3-qc). Interestingly, *bowtie2* was much more computer-intensive in both modes, perhaps because of the `--very-sensitive` option. In any case, the QC reveals a major difference in the total number of reads that are determined to be valid Hi-C contacts by `hicBuildMatrix`. As expected, mapping with *end-to-end-bowtie2* makes locating Hi-C contacts more difficult than the other methods (@fig-explorer-all-3-qc, top row), finding a very low amount of mappable, unique pairs passing the quality threshold. In contrast, mapping with *local-bowtie2* performs similarly to *bwa* in finding mappable, unique, high-quality pairs, but calls only approximately half the number of valid Hi-C contacts (\>20%), resulting in a fraction of valid Hi-C pairs that hits the expectation from *HicExplorer* docs \[ref row3\]. With *bwa*, the reads were discarded either due to low mapping quality or non-unique mates, whereas with *local-bowtie2*, the reads were almost exclusively filtered out due to low mapping quality. This must be a result of how the mappers assign mapping quality, and I believe *local-bowtie2* looks suspiciously selective in finding unique but low quality alignments. *end-to-end-bowtie* almost exclusively filters out read-pairs where one mate is unmapped, which is expected when the majority of reads are unmapped.

{{< embed ../notebooks/01_hicexplorer.ipynb#fig-explorer-all-3-qc >}}

As discussed, the five samples were pooled with `hicSumMatrices`, and the non-standard contigs (unplaced scaffolds) were filtered out, and the different resolutions were created (`hicMergeMatrixBins`). *HiCExplorer* also comes with a normalization function prior to correcting the matrix, which should be applied if different samples should have comparable bin counts. It has no effect when having only one matrix. Nevertheless, the pooled matrix was normalized and then corrected compared in @fig-explorer-pooled-norm-normcorr.

::: {#fig-explorer-pooled-norm-normcorr layout-ncol="2"}
![Normalized matrix chrX](../figures/bowtie2/local/filter_pooled_50kb_chrX.png){#fig-explorer-pooled-chrX-norm}

![Normalized and corrected chrX](../figures/bowtie2/local/normalized/normsm_filter_pooled_100kb_corrected_chrX-full.png){#fig-explorer-pooled-chrX-normcorr}

A comparison of interaction matrices before/after iterative correction (*HiCExplorer*).
:::

It is now obvious why we have to correct the matrix. The uncorrected (@fig-explorer-pooled-chrX-norm) has no signal apart from the diagonal. Even though some bins have been filtered out, the expected *plaid* pattern of a contact matrix is visible along the diagonal after the correction (@fig-explorer-pooled-chrX-normcorr), leaving evidence for chromatin structure, especially in the first 50 million bases of the chromosome. There is a wide region of empty values at the place of the centromere.

### Eigenvectors

The PCA performed by `hicPCA` on the pooled samples at both 50kb and 100kb resolution yielded the first 3 principal components. For PC1 on both resolutions (@fig-explorer-pc1-50kb, @fig-explorer-pc1-100kb) we observe only a single sign change which occurs at around 60 Mbp, the region of the centromere. It means the PCA has captured more variance between the chromosome arms than within them, making it uninformative about chromatin compartments. Upon visual inspection, it is clear that neither of the PC graphs capture the pattern of the interaction matrix by its change of sign. It seems the PCs capture variance from a bias that varies slowly and predictably along the chromosome. The first PC that is supposed to capture the compartments very suspiciously changes sign at the region of the centromere, a classic problem that could be solved by restricting the values from which the PC is calculated along the chromosome. Unimpressed, I rationalize that the option `--extra-track` to provide a gene track or histone coverage should not affect this result much. It should be provided as a phasing track to orient the eigenvector to positively correlate with gene density or histone marks, and could possibly muddle the compartments if not included. I followed *HiCExplorer* pipeline to plot and explore the matrices. At this point, I stoppped using *HiCExplorer*, as I assessed that a more flexible tool was needed.

::: {#fig-explorer-pca layout-ncol="3"}
![](../figures/bowtie2/local/normalized/pc1_50kb_corrected_chrX.png){#fig-explorer-pc1-50kb}

![](../figures/bowtie2/local/normalized/pc2_50kb_corrected_chrX.png){#fig-explorer-pc2-50kb}

![](../figures/bowtie2/local/normalized/pc3_50kb_corrected_chrX.png)

![](../figures/bowtie2/local/normalized/pc1_100kb_corrected_chrX.png){#fig-explorer-pc1-100kb}

![](../figures/bowtie2/local/normalized/pc2_100kb_corrected_chrX.png){#fig-explorer-pc2-100kb}

![](../figures/bowtie2/local/normalized/pc3_100kb_corrected_chrX.png){#fig-explorer-pc3-100kb}

Corrected interaction matrix for chromosome X along with PC1, 2, or 3, respectively. a-c: 50kb resolution, d-f: 100kb resolution. *HiCExplorer*.
:::

## Open2c ecosystem

### Quality Control

As described, the pairtools module in MultiQC [@ewels2016multiqcsummarizeanalysis] was used to visualize results from `pairtools stats` for the two parsing runs, see @fig-pairtools-qc.

#### `--walks-policy mask`

Comparing the multiQC report for each of the cell sources show similar distributions of *unmapped* (both sides unmapped), *one-sided* (one side mapped), *two-sided* (both sides mapped), and *duplicated* (w.r.t. total mapped) reads. The percentage of *cis* pairs w.r.t. mapped pairs is around 70% for all samples (@fig-pairtools-multiqc-mask-violin). The valid pairs also show similar distributions of pair types divided into 10 categories. The $P(s)$ curve looks similar for all samples as well, peaking around 250 bp separation (@fig-pairtools-multiqc-mask-ps). The QC does not show any information about mapping quality of the reads. Note that the $P(s)$ curve arise from pre-filtered pairs, meaning it provides information about the Hi-C library. As expected

#### `--walks-policy 5unique`

Parsing alignments with the recommended walks-policy aproximately halves the percentage of *unmapped* reads, and *one-* and *two-sided* reads as well *duplicated* reads are slightly increased. Overall number of unique pairs are increased with more than 20% increase. The percentage of *cis* pairs are only decreased by a percentage point at most (@fig-pairtools-multiqc-5unique-violin). Changing the walks policy does not alter the $P(s)$ curve, meaning the parameter does not bias the parsing w.r.t. genomic separation. 

::: {#fig-pairtools-qc layout-ncol="2"}

![`--walks-policy mask`](../figures/fig-pairtools-parse-multiqc-mask-violin.png){#fig-pairtools-multiqc-mask-violin}

![`--walks-policy 5unique`](../figures/fig-pairtools-parse-multiqc-5unique-violin.png){#fig-pairtools-multiqc-5unique-violin}

![`--walks-policy mask`](../figures/fig-pairtools-parse-multiqc-mask-ps.png){#fig-pairtools-multiqc-mask-ps}

![`--walks-policy 5unique`](../figures/fig-pairtools-parse-multiqc-5unique-ps.png){#fig-pairtools-multiqc-5unique-ps}

Results of `pairtools stats` run on all samples from the two walks-policies. Left (a+c): `mask`; right (b+d): `5unique`. *Generated by MultiQC* [@ewels2016multiqcsummarizeanalysis]. *Note: X-axes are not shared in the 'Genereal Statistics' plot.*  
:::

### Correction

Matrix balancing did not show major improvement in the plaid pattern, as it already showed the expected pattern. It does, however, filter out bins that are deemed too low-count to be informative, for example peri-centromeric regions. The matrix was expected to be smoother after balancing (for chromosome-wide maps), as regions along a chromosome should only vary slowly in contact frequency with other regions as they are on a continouos molecule. Therefore, sharp contrasts represent a sudden drop in bin count (@fig-rs-chrx-raw-balanced-cgi, raw) and should not be interpreted as devoid of interaction, but an indication that the data is not sufficient to interpret. It is then better to simply remove the bins in stead of correcting, which will also amplify noise. Even with a high-quality Hi-C library we expect that all bins do not have the same coverage throughout[@lajoie2015hitchhikersguidehic], as restriction enzymes do not bind equally to all regions of the genome, and therefore, some bins will be underrepresented as an artefact of binding/cutting efficieny of the restriction enzyme used.

{{< embed ../notebooks/05_rec_compartments.ipynb#fig-rs-chrx-raw-balanced-cgi >}}

{{< embed ../notebooks/05_rec_compartments.ipynb#fig-rs-chrx-raw-balanced-cgi-subset >}}

We can try to mitigate the white lines of empty bins that now appear in the matrices. The coarsegrained and interpolated matrix is useful to make a good-looking interaction matrix, but is not that useful for analysis purposes. It might get easier to visually inspect the matrix, but it is not clear how well the interpolated matrix reflects the structure of the chromatin, and it is not transparent which regions are interpolated and which that are not. I find it purposeful for interpolation on high-resolution (zoomed-in) views (@fig-rs-chrx-raw-balanced-cgi-subset) with small empty regions, but misleading for chromosome-wide maps, where typically the centromere and extremities of the chromosome have filtered-out bins. Interpolation is further discussed below.

The regions that are coarsgrained are small zero- or low-count bins which are averaged, effectively reducing the resolution of those regions until the count is sufficient. They get more frequent the longer genomic distance (the further we travel from the diagonal), and effectively enables us to get some intuition about the interactions. The coarsegrain, however, does not interpolate the `NaN`s created when filtering out whole bins in the balancing step (horisontal and vertical lines in @fig-rs-chrx-raw-balanced-cgi and @fig-rs-chrx-raw-balanced-cgi-subset; middle). This is done in a subsequent step by linearly interpolating the `NaN`s. Examining the interpolated matrix on full chrX (@fig-rs-chrx-raw-balanced-cgi; right) gives the impression that the pericentromeric (at \~60 Mbp) region harbours a *very* strong compartment, but that is clearly an artefact of the interpolation on the very large empty region of the centromere, where the diagonal is somehow extended in a square. On the thinner lines, the interpolation seem to be more smooth, and barely noticable on the diagonal.

#### `NaN` histograms

As expected, most of the low quality bins are located on the edges of the chromosome arms, especially the region around the centromere [@warren2020sequencediversityanalyses], as they contain many repetitive sequences. The low-quality bins are filtered out by the balancing algorithm, those bins are `NaN` in the Hi-C matrix. The median position of the `NaN` values (@fig-e1_nan_hist) ranges between $58$ and $63.5$, which is within the estimate of the centromeric region of *rhemac10*.

{{< embed ../notebooks/05_rec_compartments.ipynb#fig-e1_nan_hist >}}

The fact that the medians lie within the centromeric region on all cell sources shows both that the majority of the bad bins are in the (peri)centromeric region *and* there are approximately equally many on each side.

### Compartments (Eigenvectors) {#sec-results-eigenvectors}

The three viewframes (*Full*, *Arms*, *10Mb*) for the calculation of the eigenvectors captured different variability in the data (@fig-e1-matrix-full-arms-10mb-round_spermatid), and as expected, the inferred compartments (colored red on the E1 tracks) are more abundant and smaller with smaller viewframes. To determine how well each of the E1 tracks capture the pattern in the interaction matrix, we can overlay the matrix with the E1 sign-change and visually determine if the squares reflect the E1 sign change (@fig-e1-matrix-full-arms-10mb-round_spermatid).

{{< embed ../notebooks/07_various_plotting.ipynb#fig-e1-matrix-full-arms-10mb-round_spermatid >}}

I decide that without more finescaled knowledge than the position of the centromeres, the arbitrary size of the 10 Mb windowed E1 can not fully be justified. That is, we could arbitrarily calculate any windowed E1 track. Also, @wang2019reprogrammingmeioticchromatin concludes only for pachytene spermatocyte to show local interactions in the 10Mb viewframe (what they refer to as *refined A/B-compartments*), and all the other stages of spermatogenesis were consistent with the conventional A/B compartments. The reasonable thing to do is therefore to continue the analysis, focusing on the arms-restricted eigendecomposition. Nevertheless, we also keep *refined* compartments in the analysis.

Additionally, as I created coolers with two different sets of parsing parameters we will compare the resulting matrices and their compartments (@fig-rs100-recpe-pe). As expected, we observe more empty bins in the Hi-C matrix when comparing the initial run (`mask`) to the recommended parameters (`5unique`), but otherwise, the interaction pattern is indestinguishable. The effect on the E1 is more noticable, where the absolute magnitude of the E1 values is generally smaller. There is, however, a small region that changes sign (from A to B) on the 10Mb-windowed ('refined') E1 track (@fig-rs100-recpe-pe;c+d). This region is surrounded by added empty bins, which could mean that too many low quality pairs in `mask` were introducing bias and swapped the sign of E1. It is supported by the fact that the sign change *only* occured in *refined* E1, and that the sign after filtering weak pairs ($mapq < 30$) is consistent with the *arms* view. It supports my previous postulate that it is better to use a viewframe with explicit molecular meaning than one of an arbitrary window size. That said, the `mapq` threshold should really be determined taking both coverage and resolution into account. For our purposes, and with the *arms* view, the mapping- and parsing parameters do not seem to be too sensitive.

{{< embed ../notebooks/07_various_plotting.ipynb#fig-rs100-recpe-pe >}}

To emphasize the findings, the sets of A-compartments were compared between the two parsing runs, showing almost identical compartment calls. Additionally, the set difference was 8 bins between PE and recPE for round spermatid 100kb and 5 bins for fibroblast for *arms* viewframe (@fig-rs-fb-100-pe-recpe-intervals; a+b, respectively). We observe a high number of differences around 76Mb for the refined compartments (10Mb) of round spermatid, which is consistent with the sign-flip of E1 values discussed earlier. Anything else would be surprising, as it is the same data, but visualized in a different way.

{{< embed ../notebooks/07_various_plotting.ipynb#fig-rs-fb-100-pe-recpe-intervals >}}

The observed difference between the sets can for our data be attributed to chance, but we cannot draw general conclusions about the parameters in general. I argue that the quality and size of the Hi-C library will influence sensitive to parsing parameters. In that case, the most flexible approach is still to follow the recommendations from `cooler` to report more pairs as valid contacts, and then create coolers with different *mapq* filters if issues are encountered.

### Compartment Edges (transition zones)

We compare how the ECH90 regions fit when queried on top of the A-compartments and equivalently for the edges, for fibroblasts and round spermatids at 100kb resolution. When queried against the edges in stead, the the total set size is reduced to less than 50%. Interestingly, some of the intersections between A-compartments and ECH90 remain, and new ones appear as we move to the outside edge of the compartment (@fig-comps-edges-ech). This indicates that most, but not all, of the intersection between ECH90 regions and the A-compartments are within 100kb of the compartment edge, and additional overlap is gained if we define a transition zone on the outside of the edge as well. To visualize this (outside) edge enrichment, we find the set difference of the ECH-intersection to compartments and edges, respectively (@fig-edge-enrichment), thus removing all the 'inside' edges. We observe that in almost all of the of the regions of $ECH \cap Comp$ are accompanied by an edge also intersecting ECH ($ECH \cap Edge$), localized where the *Diff* track aligns (within 100kb) with both $CompInt$ and $EdgeInt$.

{{< embed ../notebooks/07_various_plotting.ipynb#fig-comps-edges-ech >}}

{{< embed ../notebooks/07_various_plotting.ipynb#fig-edge-enrichment >}}

We apply both proximity test and Jaccard test, to see how well the results could occur by chance (@fig-proximity-jaccard-bar). For completeness, the tests are included for all cell types, but we only use 100kb resolution arms viewframe. We observe that both fibroblast and round spermatid have $p < 0.05$ for both tests, meaning the two cell type have both more intersection with ECH regions than expected by chance (Jaccard) *and* the non-overlapping intervals are more proximal to compartment edges than expected by chance (proximity test). I argue that a significant Jaccard statistic should be interpreted as a significant amount of overlap between the two sets, i.e. compartment edges and ECH90 regions, and the proximity test (when performed on the edges) gives us information about the potential of expanding or moving the transition window. That is, if the non-overlapping regions are *very* proximal, a larger (or shifted) window to only capture the 200kb region outside of the edge might be favourable.

{{< embed ../notebooks/06_rec_genomicintervals.ipynb#fig-proximity-jaccard-bar >}}

## Testing against regions of selection in baboons

The data for this analysis was provided by Kasper Munch in bed-like format, mapped to *panu_3.0* (PapAnu4) assembly. The intervals define genomic regions in a hybrid/migrating population of baboon where strong negative selection acts against minor parent ancestry [@sorensen2023genomewidecoancestryreveals]. The segments had to be lifted to rheMac10 to be able to correlate the two sets of intervals. The original UCSC liftOver [@hinrichs2006ucscgenomebrowser] is very strict and does not try to conserve segments in favor of accuracy e.g. inversions or small indels, which results in highly scattered regions when lifted. To favor preservation of segments, we use *segment_liftover* [@gao2018segment_liftoverpythontool], resulting in much more similar regions to the original (@fig-compare-liftover). As no chain file from *panu_3.0* to *Mmul_10* was available, we had to use *panu_2.0* as intermediate. 

{{< embed ../notebooks/07_various_plotting.ipynb#fig-compare-liftover >}}

Initially, the compartment edges of round spermatid at 100kb resolution (RS100) were plotted against the lifted coordinates from a *P.* anubis-hamadryas hybrid population, where either all the sampled individuals have *Papio anubis* ancestry or 95% of the sampled individuals have *Papio hamadryas* ancestry. Their respective intersections were plotted undeneath. We expect less intersection for *hamadryas* than for *anubis* as the total set size is much smaller. The compartment edges and *Papio anubis*-derived regions(@fig-baboon-rs100-intersect; b) seem to be highly enriched in the first 25 Mbp, and thus it has a high degree of intersection with the compartment edges. Interestingly, the ECH90 set is nearly empty in that region, making the finding outside the scope of this analysis, although it could be useful for determining the mechanism for selecting the *P.anubis* ancestral allele in the hybrid baboon population. The *P.hamadryas*-derived regions seem intersect the compartment edges more centered on the chromosome (@fig-baboon-rs100-intersect; a). The proximity test ruled out that the non-intersecting parts of the respective regions were this proximal by chance. However, the Jaccard test revealed that the intersection between the RS100 and both Hi-*P.hama* and Hi-*P.anu* can be explained by chance alone .

{{< embed ../notebooks/07_various_plotting.ipynb#fig-baboon-rs100-intersect >}}

{{< pagebreak >}}

# Discussion

Here is the discussion

# Bibliography {.unnumbered}

\begingroup
\raggedright

::: {#refs}
:::

\endgroup